{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49808257",
   "metadata": {},
   "source": [
    "## **Serving an Agent with FastAPI**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d097cdd6",
   "metadata": {},
   "source": [
    "### **What is FastAPI and Why Use It for Agents?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55bda085",
   "metadata": {},
   "source": [
    "FastAPI is a modern, high-performance web framework for building APIs with Python. Released in 2018, it has quickly gained popularity due to its combination of speed, ease of use, and developer-friendly features.\n",
    "\n",
    "As its core, FastAPI is design to create REST APIs that can serve requests efficiently while providing robust validation and documentation. For AI Agent deployment, FastAPI offers several critical advantages:\n",
    "- **Asynchronous Support**: AI agents often need to handle concurrent requests efficiently. FastAPI's native async/await support enables handling thousands of simultaneous connections, perfect for serving multiple agent requests in parallel without blocking.\n",
    "- **Streaming Responses**: Agents frequently generate content incrementally (token by token). FastAPI's streaming response capabilities allow for real-time transmission of agent outputs as they're generated, creating a more responsive user experience.\n",
    "- **Type Validation**: When working with agents, ensuring proper input formats is curcial. FastAPI uses Pydantic for automatic request validation, catching malformed inputs before they reach your agent and providing clear error messages.\n",
    "- **Performance**: Built on Starlette and Uvicorn, FastAPI offers near-native performance. For compute-intensive agent applications, this means your infrastructure handles API overhead efficiently, allowing more resources for the actual agent processing.\n",
    "- **Automatic Documentation**: When exposing an agent API to multiple users or teams, documentation becomes essential. FastAPI automatically generates interactive API documentation via Swagger UI and ReDoc, making it easy for others to understand and use your agent.\n",
    "- **Schema Enforcement**: Pydantic models ensure that both requests to your agent and reponses from it conform to predefined schemas, making agent behavior more predictable and easier to integrate with other systems.\n",
    "\n",
    "In this tutorial, we'll build a complete API that serves an AI Agent with both synchronous and streaming endpoints, demonstrating how FastAPI's features address the specific challenges of deploying agents in productions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532e8326",
   "metadata": {},
   "source": [
    "### **Prerequisite**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "feed35d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv add fastapi uvicorn pydantic python-dotenv sse-starlette --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f9797f",
   "metadata": {},
   "source": [
    "### **Agent Quick Recap**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6329bf",
   "metadata": {},
   "source": [
    "Let's start by defining a simple agent that we'll expose via our API. This could be any agent implementation, but for this tutorial, we'll create a basic example that simylates an AI agent responding to user queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e986e524",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleAgent:\n",
    "    def __init__(self, name=\"FastAPI Agent\"):\n",
    "        self.name = name\n",
    "\n",
    "    def generate_response(self, query):\n",
    "        \"\"\"Generate a synchronous response to a user query.\"\"\"\n",
    "        return f\"Agent {self.name} received: '{query}'\\nResponse: This is a simulated agent response.\"\n",
    "\n",
    "    async def generate_response_stream(self, query):\n",
    "        \"\"\"Generate a streaming response to a user query.\"\"\"\n",
    "\n",
    "        import asyncio\n",
    "\n",
    "        prefix = f\"Agent {self.name} received: '{query}'\\n\"\n",
    "        response = \"This is a simulated agent response that streams token by token.\"\n",
    "\n",
    "        # Yield the prefix as a single chunk\n",
    "        yield prefix\n",
    "\n",
    "        # Stream the response token by token with small delays\n",
    "        for token in response.split():\n",
    "            await asyncio.sleep(0.1)\n",
    "            yield token + \" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb3940e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent FastAPI Agent received: 'Hi there, how are you?'\n",
      "Response: This is a simulated agent response.\n"
     ]
    }
   ],
   "source": [
    "agent = SimpleAgent()\n",
    "test_query = \"Hi there, how are you?\"\n",
    "print(agent.generate_response(test_query))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5174ddc",
   "metadata": {},
   "source": [
    "### **Minial FastAPI App**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e30842b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "\n",
    "# Initialize FastAPI app\n",
    "app = FastAPI(\n",
    "    title = \"Agent API\",\n",
    "    description = \"A simple API that serves an AI agent.\",\n",
    "    version = \"0.0.1\"\n",
    ")\n",
    "\n",
    "# Create an instance of the agent\n",
    "agent = SimpleAgent()\n",
    "\n",
    "# Create a simple endpoint\n",
    "@app.get(\"/\")\n",
    "def health_check():\n",
    "    \"\"\"Check if the API is running\"\"\"\n",
    "    return {\"status\": \"ok\", \"message\": \"API is operational\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d22e0e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "import uvicorn\n",
    "\n",
    "def run_server(port):\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=port)\n",
    "\n",
    "def run_main(port):\n",
    "    thread = threading.Thread(target=run_server, daemon=True, args=[port])\n",
    "    thread.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1c41e1",
   "metadata": {},
   "source": [
    "### POST /agent - Synchronous Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "809115e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, ConfigDict\n",
    "from typing import Optional\n",
    "\n",
    "# Define request and response models\n",
    "class QueryRequest(BaseModel):\n",
    "    query: str\n",
    "    context: Optional[str] = None\n",
    "\n",
    "    model_config = ConfigDict(\n",
    "        json_schema_extra = {\n",
    "            \"example\": [\n",
    "                {\n",
    "                    \"query\": \"What is FastAPI?\",\n",
    "                    \"context\": \"I'm a beginner programmer.\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    )\n",
    "\n",
    "class QueryResponse(BaseModel):\n",
    "    response: str\n",
    "\n",
    "    model_config = ConfigDict(\n",
    "        json_schema_extra = {\n",
    "            \"example\": [\n",
    "                {\n",
    "                    \"response\": \"FastAPI is a modern, high-performance web framework for building APIs with Python.\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "814ef211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a synchronous endpoint for the agent\n",
    "@app.post(\"/agent\", response_model=QueryResponse)\n",
    "def query_agent(request: QueryRequest):\n",
    "    \"\"\"Get a reponse from agent\"\"\"\n",
    "    response = agent.generate_response(request.query)\n",
    "    return QueryResponse(response=response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9587b72f",
   "metadata": {},
   "source": [
    "This endpoint accepts POST requests with a JSON body containing a \"query\" field and an optional \"context\" field. It returns a JSON response with the agent's answer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40425c4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [3868]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://0.0.0.0:5001 (Press CTRL+C to quit)\n"
     ]
    }
   ],
   "source": [
    "run_main(5001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fba7c81",
   "metadata": {},
   "source": [
    "### **POST /agent/stream - Token Streaming**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61a53414",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi.responses import StreamingResponse\n",
    "import json\n",
    "\n",
    "@app.post(\"/agent/stream\")\n",
    "async def stream_agent(request: QueryRequest):\n",
    "    \"\"\"Stream a response from the agent token by token\"\"\"\n",
    "\n",
    "    async def event_generator():\n",
    "        async for token in agent.generate_response_stream(request.query):\n",
    "            # Format in JSON Object\n",
    "            data = json.dumps({\"token\": token})\n",
    "            yield f\"data: {data}\\n\\n\"\n",
    "\n",
    "    return StreamingResponse(\n",
    "        event_generator(),\n",
    "        media_type=\"text/event-stream\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "419eba8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [3868]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://0.0.0.0:5002 (Press CTRL+C to quit)\n"
     ]
    }
   ],
   "source": [
    "run_main(5002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ee31ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sse_starlette.sse import EventSourceResponse\n",
    "\n",
    "@app.post(\"/agent/stream-sse\")\n",
    "async def stream_agent_sse(request: QueryRequest):\n",
    "    \"\"\"Stream a response using SSE with the sse-starlette package\"\"\"\n",
    "\n",
    "    async def event_generator():\n",
    "        async for token in agent.generate_response_stream(request.query):\n",
    "            yield {\"data\": json.dumps({\"token\": token})}\n",
    "            \n",
    "    return EventSourceResponse(event_generator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bbcc86cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [3868]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://0.0.0.0:5003 (Press CTRL+C to quit)\n"
     ]
    }
   ],
   "source": [
    "run_main(5003)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d436d5b8",
   "metadata": {},
   "source": [
    "### **Creating the full application**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19782e1f",
   "metadata": {},
   "source": [
    "#### 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "44ab2a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, Depends, HTTPException, Header\n",
    "from fastapi.responses import StreamingResponse\n",
    "from pydantic import BaseModel, ConfigDict\n",
    "from typing import Optional\n",
    "import json\n",
    "import os\n",
    "import asyncio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e8d512",
   "metadata": {},
   "source": [
    "#### 2. Define Agent Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "078abf36",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleAgent:\n",
    "    \n",
    "    def __init__(self, name=\"FastAPI Agent\"):\n",
    "        self.name = name\n",
    "\n",
    "    def generate_response(self, query):\n",
    "        \"\"\"Generate a synchronous response to a user query\"\"\"\n",
    "        return f\"Agent {self.name} received: '{query}'\\nResponse: This is a simulated agent response.\"\n",
    "    \n",
    "    async def generate_reponse_stream(self, query):\n",
    "        \"\"\"Generate a streaming reponse to a user query\"\"\"\n",
    "        \n",
    "        prefix = f\"Agent {self.name} is thinking about: '{query}'\\n\"\n",
    "        response = \"This is a simulated agent reponse that streams token by token.\"\n",
    "\n",
    "        yield prefix\n",
    "\n",
    "        for token in response.split():\n",
    "            await asyncio.sleep(0.1)\n",
    "            yield token + \" \""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e295447c",
   "metadata": {},
   "source": [
    "#### 3. Define the request and response models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "35e96e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryRequest(BaseModel):\n",
    "    query: str\n",
    "    context: Optional[str] = None\n",
    "\n",
    "    model_config = ConfigDict(\n",
    "        json_schema_extra={\n",
    "            \"example\": [\n",
    "                {\n",
    "                    \"query\": \"What is FastAPI?\",\n",
    "                    \"context\": \"I'm a beginner programmer.\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    )\n",
    "\n",
    "class QueryResponse(BaseModel):\n",
    "    response: str\n",
    "\n",
    "    model_config = ConfigDict(\n",
    "        json_schema_extra={\n",
    "            \"example\": [\n",
    "                {\n",
    "                    \"response\": \"FastAPI is a modern, high-performance web framework for building APIs with Python.\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81f1eeb",
   "metadata": {},
   "source": [
    "#### 4. Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4bfa11ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = FastAPI(\n",
    "    title=\"Agent API\",\n",
    "    description=\"A simple API that serves an AI agent\",\n",
    "    version=\"0.1.0\"\n",
    ")\n",
    "\n",
    "agent = SimpleAgent()\n",
    "\n",
    "# Create endpoints\n",
    "@app.get(\"/health\")\n",
    "def health_check():\n",
    "    \"\"\"Check if the API is running\"\"\"\n",
    "    return {\"status\": \"ok\", \"message\": \"API is operational\"}\n",
    "\n",
    "@app.post(\"/agent\", response_model=QueryResponse)\n",
    "def query_agent(request: QueryRequest):\n",
    "    \"\"\"Get a response from user input\"\"\"\n",
    "    response = agent.generate_response(request.query)\n",
    "    return QueryResponse(response=response)\n",
    "\n",
    "@app.post(\"/agent/stream\", response_model=QueryResponse)\n",
    "async def stream_agent(request: QueryRequest):\n",
    "    \"\"\"Stream a response from the agent token by token\"\"\"\n",
    "    \n",
    "    async def event_generator():\n",
    "        async for token in agent.generate_reponse_stream(request.query):\n",
    "            data = json.dumps({\"token\": token})\n",
    "            yield f\"data: {data}\\n\"\n",
    "\n",
    "    return StreamingResponse(\n",
    "        event_generator(),\n",
    "        media_type=\"text/event-stream\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bdaceee",
   "metadata": {},
   "source": [
    "#### 5. Running Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effdb6c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [3868]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://0.0.0.0:5007 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:57961 - \"POST /agent HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:57983 - \"POST /agent/stream HTTP/1.1\" 200 OK\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "import uvicorn\n",
    "\n",
    "def run_server(port):\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=port)\n",
    "\n",
    "def run_main(port):\n",
    "    thread = threading.Thread(target=run_server, daemon=True, args=[port])\n",
    "    thread.start()\n",
    "\n",
    "run_main(5007)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3d93eb",
   "metadata": {},
   "source": [
    "### **Simple Client Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8b06a25d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synchronous Response:\n",
      "{'response': \"Agent FastAPI Agent received: 'What is FastAPI?'\\nResponse: This is a simulated agent response.\"}\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "Streaming Response:\n",
      "Agent FastAPI Agent is thinking about: 'Tell me about streaming'\n",
      "This is a simulated agent reponse that streams token by token. "
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# Test the synchronous endpoint\n",
    "response = requests.post(\n",
    "    \"http://localhost:5007/agent\", \n",
    "    json={\"query\": \"What is FastAPI?\"}\n",
    ")\n",
    "print(\"Synchronous Response:\")\n",
    "print(response.json())\n",
    "print(\"\\n\" + \"-\" * 40 + \"\\n\")\n",
    "\n",
    "# Test the streaming endpoint\n",
    "response = requests.post(\n",
    "    \"http://localhost:5007/agent/stream\",\n",
    "    json={\"query\": \"Tell me about streaming\"},\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "print(\"Streaming Response:\")\n",
    "for line in response.iter_lines():\n",
    "    if line:\n",
    "        # Parse the SSE format\n",
    "        line = line.decode('utf-8')\n",
    "        if line.startswith('data: '):\n",
    "            data = json.loads(line[6:])\n",
    "            print(data[\"token\"], end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d46c30",
   "metadata": {},
   "source": [
    "### **Adding Basic Auth Key (Optional)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6dc2243e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import Depends, HTTPException, Header\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "os.environ[\"API_KEY\"] = os.getenv(\"TEST_API_KEY\")\n",
    "\n",
    "# Function to validate the API key\n",
    "async def verify_api_key(x_api_key: str = Header(None)):\n",
    "    \"\"\"Verify the API key provided in the X-API-Key header\"\"\"\n",
    "\n",
    "    api_key = os.environ.get(\"API_KEY\")\n",
    "\n",
    "    if not api_key:\n",
    "        return True\n",
    "    \n",
    "    if not x_api_key:\n",
    "        raise HTTPException(status_code=401, detail=\"API_KEY is missing\")\n",
    "    \n",
    "    if x_api_key != api_key:\n",
    "        raise HTTPException(status_code=403, detail=\"API_KEY is invalid\")\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d27df3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update endpoints to include the API key dependency\n",
    "@app.post(\"/agent\", response_model=QueryResponse)\n",
    "def query_agent(request: QueryRequest, auth: bool = Depends(verify_api_key)):\n",
    "    \"\"\"Get a reponse from agent\"\"\"\n",
    "\n",
    "    response = agent.generate_response(request.query)\n",
    "    return QueryResponse(response=response) \n",
    "\n",
    "@app.post(\"/agent/stream\")\n",
    "async def stream_agent(request: QueryRequest, auth: bool = Depends(verify_api_key)):\n",
    "    \"\"\"Stream a response from the agent token by token\"\"\"\n",
    "\n",
    "    async def event_generator():\n",
    "        async for token in agent.generate_reponse_stream(request.query):\n",
    "            data = json.dumps({\"token\": token})\n",
    "            yield f\"data: {data}\\n\"\n",
    "\n",
    "    return StreamingResponse(\n",
    "        event_generator(),\n",
    "        media_type=\"text/event-stream\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4b7003",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [3868]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://0.0.0.0:5011 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:58009 - \"POST /agent HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:58011 - \"POST /agent/stream HTTP/1.1\" 200 OK\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "import uvicorn\n",
    "\n",
    "def run_server(port):\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=port)\n",
    "\n",
    "def run_main(port):\n",
    "    thread = threading.Thread(target=run_server, daemon=True, args=[port])\n",
    "    thread.start()\n",
    "\n",
    "run_main(5011)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bbef1509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synchronous Response:\n",
      "{'response': \"Agent FastAPI Agent received: 'What is FastAPI?'\\nResponse: This is a simulated agent response.\"}\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "Streaming Response:\n",
      "Agent FastAPI Agent is thinking about: 'Tell me about streaming'\n",
      "This is a simulated agent reponse that streams token by token. "
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# Test the synchronous endpoint\n",
    "response = requests.post(\n",
    "    \"http://localhost:5011/agent\", \n",
    "    json={\"query\": \"What is FastAPI?\"}\n",
    ")\n",
    "print(\"Synchronous Response:\")\n",
    "print(response.json())\n",
    "print(\"\\n\" + \"-\" * 40 + \"\\n\")\n",
    "\n",
    "# Test the streaming endpoint\n",
    "response = requests.post(\n",
    "    \"http://localhost:5011/agent/stream\",\n",
    "    json={\"query\": \"Tell me about streaming\"},\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "print(\"Streaming Response:\")\n",
    "for line in response.iter_lines():\n",
    "    if line:\n",
    "        # Parse the SSE format\n",
    "        line = line.decode('utf-8')\n",
    "        if line.startswith('data: '):\n",
    "            data = json.loads(line[6:])\n",
    "            print(data[\"token\"], end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cebdb95",
   "metadata": {},
   "source": [
    "### **Unit Tests**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c96e5d9",
   "metadata": {},
   "source": [
    "### **Conclusions**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d529d6d2",
   "metadata": {},
   "source": [
    "In this tutorial, we've built a FastAPI application that serves a simple AI Agent with both synchronous and streaming endpoints. We've covered the basics of setting up FastAPI, defining Pydantic models for request/response validation, implementing both synchronous and streaming endpoints, and adding simple authentication.\n",
    "\n",
    "FastAPI's combination of performance, automatic documentation, and developer-friendly features makes it an excellent choice for serving AI agents in production. By following the patterns in this tutorial, you can create robust, production-ready APIs for your own AI agents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4672af4b",
   "metadata": {},
   "source": [
    "### **Next steps**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9843ac49",
   "metadata": {},
   "source": [
    "Now that you have a basic FastAPI agent service running, here are some ideas for next steps:\n",
    "- **Add more advanced agents**: Replace the simple agent with your production-ready agent\n",
    "- **Implement authentication and rate limiting**: Add more sophisticated authentication and rate limiting for production use\n",
    "- **Add middleware for logging and monitoring**: Implement middleware for request logging and performance monitoring\n",
    "- **Set up deployment**: Deploy your FastAPI application to a production enviroment using Docker, Kubernetes, or a cloud service\n",
    "- **Implement async database connections**: Add database integrations for storing conversation history or other data\n",
    "- **Add background tasks**: Use FastAPI's background tasks for long-running operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49571b3",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-agent-tutorials",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
