{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "787ac091",
   "metadata": {},
   "source": [
    "## **Agent Memory with Redis**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb6c423",
   "metadata": {},
   "source": [
    "### **I. Introduction**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c0379f",
   "metadata": {},
   "source": [
    "Without memory, AI agents are like goldfish - they forget everything after each conversation and can't learn from past interactions or maintain context across sessions. Agentic systems require both **short-term** and **long-term** memory in order to complete tasks in a personalized and resilient manner. Memory is all about state management and `Redis` is the well-known in-memory database for exactly this kind of use case today in production systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617994f5",
   "metadata": {},
   "source": [
    "### **II. What we'll build**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ef17ea",
   "metadata": {},
   "source": [
    "This tutorial demonstrates how to build a **memory-enabled travel agent** with **Redis and LangGraph** that remembers user preferences and provides personalized recommendations. This is a horizontal concept that you can take and apply to your own agent use cases.\n",
    "\n",
    "We'll explore:\n",
    "1. Short-term memory management using LangGraph's checkpointer\n",
    "2. Long-term memory storage and retrieval using RedisVL\n",
    "3. Managing long-term memory as a tool for a ReAct agent\n",
    "4. Managing conversation history size with summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8c44b9",
   "metadata": {},
   "source": [
    "### **III. Memory architecture overview**\n",
    "\n",
    "Ouer agent uses a dual-memory system:\n",
    "* **Short-term**: Manages conversation context\n",
    "* **Long-term**: Stores persistent knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac802d5d",
   "metadata": {},
   "source": [
    "#### **1. Short-term Memory**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13d1e34",
   "metadata": {},
   "source": [
    "The agent tracks chat history using Redis through LangGraph's checkpointer. Each node in the graph (Retrieve Memories, Respond, Summarize) saves its state to Redis, including conversation history and thread metadata."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96208d83",
   "metadata": {},
   "source": [
    "<img src=\"short-term-memory.png\" alt=\"alt text\" width=\"400px\" height=\"400px\" style=\"background-color: lightblue;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b846e7c1",
   "metadata": {},
   "source": [
    "To prevent context window pollution, the agent summarizes conversations when they exceed a configurable length."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d5b81d",
   "metadata": {},
   "source": [
    "#### **2. Long-term Memory**\n",
    "\n",
    "Long-term memories are stored & indexed in Redis using the RedisVL client, with 2 types:\n",
    "* **Episodic**: User preferences and experiences\n",
    "* **Semantic**: General travel knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5594b3b7",
   "metadata": {},
   "source": [
    "<img src=\"long-term-memory.png\" alt=\"alt text\" width=\"500px\" height=\"300px\" style=\"background-color: lightblue;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058a3722",
   "metadata": {},
   "source": [
    "### **IV. Set up enviroment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84c977d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2mResolved \u001b[1m145 packages\u001b[0m \u001b[2min 17ms\u001b[0m\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m142 packages\u001b[0m \u001b[2min 39ms\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv add langchain-google-genai langgraph-checkpoint langgraph langgraph-checkpoint-redis langchain-redis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b937f7e4",
   "metadata": {},
   "source": [
    "#### **1. Required API keys**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aff84850",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "def _set_env(key: str):\n",
    "    if key not in os.environ:\n",
    "        os.environ[key] = getpass.getpass(f\"Enter your {key}: \")\n",
    "\n",
    "_set_env(\"GOOGLE_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1e0984",
   "metadata": {},
   "source": [
    "#### **2. Setup Redis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a768fb3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from redis import Redis\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "redis_host = os.getenv(\"REDIS_HOST\")\n",
    "redis_port = os.getenv(\"REDIS_PORT\")\n",
    "redis_username = os.getenv(\"REDIS_USERNAME\")\n",
    "redis_password = os.getenv(\"REDIS_PASSWORD\")\n",
    "\n",
    "redis_client = Redis(\n",
    "    host = redis_host,\n",
    "    port = redis_port,\n",
    "    decode_responses = True,\n",
    "    username = redis_username,\n",
    "    password = redis_password\n",
    ")\n",
    "\n",
    "redis_client.ping()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f5f9a8",
   "metadata": {},
   "source": [
    "#### **3. Prepare memory data models**\n",
    "\n",
    "In this section, we'll create a robust data modeling system for our agent's memory using `Pydantic`. These models will ensure type safety and provide clear data strutures for storing and retrieving memories form Redis.\n",
    "\n",
    "We'll implement 4 key components:\n",
    "1. `MemoryType` - An enumeration that categorizes memories into 2 types:\n",
    "* Episodic: Personal experiences and user preferences\n",
    "* Semantic: General knowledge and domain facts\n",
    "2. `Memory` - The core model representing a single memory entry with its content and metadata\n",
    "3. `Memories` - A container model that holds collections of memory objects\n",
    "4. `StoredMemory` - A specialized model for memories that have been persistend to Redis\n",
    "\n",
    "These models work together to create a complete memory lifecycle, from creation to storage and retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c97d615",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ulid\n",
    "\n",
    "from datetime import datetime\n",
    "from enum import Enum\n",
    "from typing import List, Optional\n",
    "from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36c68805",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryType(str, Enum):\n",
    "    \"\"\"\n",
    "    Defines the type of long-term memory for categorization and retrieval.\n",
    "\n",
    "    EPISODIC: Personal experiences and user-specific preferences\n",
    "            (e.g., \"User prefers Delta airlines\", \"User visited Paris last year\")\n",
    "    \n",
    "    SEMANTIC: General domain knowledge and facts\n",
    "            (e.g., \"Singapore requires passport\", \"Tokyo has excellent public transit\")\n",
    "\n",
    "    The type of a long-term memory.\n",
    "\n",
    "    EPISODIC: User specific experiences and preferences\n",
    "\n",
    "    SEMANTIC: General on top of the user's preferences and LLM's training data.\n",
    "    \"\"\"\n",
    "\n",
    "    EPISODIC = \"episodic\"\n",
    "    SEMANTIC = \"semantic\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a538773",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory(BaseModel):\n",
    "    \"\"\"Represents a single long-term memory.\"\"\"\n",
    "\n",
    "    content: str\n",
    "    memory_type: MemoryType\n",
    "    metadata: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1cc2eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memories(BaseModel):\n",
    "    \"\"\"A list of memories extracted from a conversation by an LLM.\"\"\"\n",
    "\n",
    "    memories: List[Memory]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "857719f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StoredMemory(Memory):\n",
    "    \"\"\"A stored long-term memory\"\"\"\n",
    "\n",
    "    id: str # The redis key\n",
    "    memory_id: ulid.ULID = Field(default_factory=lambda: ulid.ULID())\n",
    "    created_at: datetime = Field(default_factory=datetime.now)\n",
    "    user_id: Optional[str] = None\n",
    "    thread_id: Optional[str] = None\n",
    "    memory_type: Optional[MemoryType] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6756fd",
   "metadata": {},
   "source": [
    "Now we have type-safe data models that handle the complete memory lifecycle from LLM extraction to Redis storage, with proper metadata tracking for production use. Next, we'll set up the Redis infrastructure to store and search these memories using vector embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1398f4b8",
   "metadata": {},
   "source": [
    "### **V. Memory Storage**\n",
    "\n",
    "- **Short-term memory** is handled automatically by `RedisSaver` from `langgraph-checkpoint-redis`\n",
    "- **Long-term memory**, we'll use RedisVL with vector embeddings to enable semantic search of past experiences and knowledge.\n",
    "\n",
    "Below, we'll create a search index schema in Redis to hold our long term memories. The schema has a few different fields including content, memory type, metadata, timestamps, user id, memory id, and the embedding of the memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a35893a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from redisvl.index import SearchIndex\n",
    "from redisvl.schema.schema import IndexSchema\n",
    "\n",
    "# Define the schema for our vector search index\n",
    "# This creates the structure for storing and query memories\n",
    "memory_schema = IndexSchema.from_dict({\n",
    "    \"index\": {\n",
    "        \"name\": \"agent_memories\", # Index name for identification\n",
    "        \"prefix\": \"memory\", # Redis key prefix (memory:1, memory:2, ...)\n",
    "        \"key_separator\": \":\",\n",
    "        \"storage_type\": \"json\"\n",
    "    },\n",
    "    \"fields\": [\n",
    "        {\"name\": \"content\", \"type\": \"text\"},\n",
    "        {\"name\": \"memory_type\", \"type\": \"tag\"},\n",
    "        {\"name\": \"metadata\", \"type\": \"text\"},\n",
    "        {\"name\": \"created_at\", \"type\": \"text\"},\n",
    "        {\"name\": \"user_id\", \"type\": \"tag\"},\n",
    "        {\"name\": \"memory_id\", \"type\": \"tag\"},\n",
    "        {\n",
    "            \"name\": \"embedding\", \n",
    "            \"type\": \"vector\",\n",
    "            \"attrs\": {\n",
    "                \"algorithm\": \"flat\",\n",
    "                \"dims\": 768,\n",
    "                \"distance_metric\": \"cosine\",\n",
    "                \"datatype\": \"float32\"\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc401701",
   "metadata": {},
   "source": [
    "Below we create the `SearchIndex` from the `IndexSchema` and our Redis client connection object. We'll overwrite the index spec if its already created!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "19220e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long-term memory index ready\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    long_term_memory_index = SearchIndex(\n",
    "        schema = memory_schema,\n",
    "        redis_client = redis_client,\n",
    "        validate_on_load = True\n",
    "    )\n",
    "\n",
    "    long_term_memory_index.create(overwrite = True)\n",
    "    print(\"Long-term memory index ready\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating index: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de45003a",
   "metadata": {},
   "source": [
    "#### **1. Functions to access memories**\n",
    "\n",
    "Next, we provide 3 core functions to access, store and retrieve memories. We'll eventually use these in tools for the LLM to call. We'll start by loading a vectorizer class to create GEMINI embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "87fc68db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai.embeddings import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "gg_embed = GoogleGenerativeAIEmbeddings(\n",
    "    model = 'text-embedding-004',\n",
    "    api_key = os.environ[\"GOOGLE_API_KEY\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029de9a2",
   "metadata": {},
   "source": [
    "Next we'll set up a simple logger so our functions will record log activity of what's happening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9414c602",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee2b4b3",
   "metadata": {},
   "source": [
    "##### **a. Check for similar memories**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8cd0af",
   "metadata": {},
   "source": [
    "First, we'll write a utility function to check if a memory similar to a given memory already exists in the index.\n",
    "\n",
    "This function checks for duplicate memories in Redis by:\n",
    "1. Converting the input content into a vector embedding\n",
    "2. Creating filters for user_id and memory_type\n",
    "3. Using vector similarity search with a vector range query to find any existing + similar memories\n",
    "4. Returning True if a similar memory exists, False otherwise\n",
    "\n",
    "This helps prevent storing redundant information in the agent's memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "18e34b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from redisvl.query import VectorRangeQuery\n",
    "from redisvl.query.filter import Tag\n",
    "\n",
    "# If we have any memories that aren't associated with a user, we'll use this ID\n",
    "SYSTEM_USER_ID = \"system\"\n",
    "\n",
    "def similar_memory_exists(\n",
    "    content: str,\n",
    "    memory_type: MemoryType,\n",
    "    user_id: str = SYSTEM_USER_ID,\n",
    "    thread_id: Optional[str] = None,\n",
    "    distance_threshold: float = 0.1,\n",
    ") -> bool:\n",
    "    \"\"\"Check if a similar long-term memory already exists in Redis\"\"\"\n",
    "    content_embedding = gg_embed.embed_documents([content])\n",
    "    content_embedding = content_embedding[0]\n",
    "\n",
    "    filters = (Tag(\"user_id\") == user_id) & (Tag(\"memory_type\") == memory_type)\n",
    "\n",
    "    if thread_id:\n",
    "        filters = filters & (Tag(\"thread_id\") == thread_id)\n",
    "\n",
    "    vector_query = VectorRangeQuery(\n",
    "        vector = content_embedding,\n",
    "        num_results = 1,\n",
    "        vector_field_name = \"embedding\",\n",
    "        filter_expression = filters,\n",
    "        distance_threshold = distance_threshold,\n",
    "        return_fields = [\"id\"]\n",
    "    )\n",
    "\n",
    "    results = long_term_memory_index.query(vector_query)\n",
    "    logger.debug(f\"Similar memory search results: {results}\")\n",
    "\n",
    "    if results:\n",
    "        logger.debug(\n",
    "            f\"{len(results)} similar {'memory' if results.count == 1 else 'memories'} found. First: \"\n",
    "            f\"{results[0]['id']}. Skipping storage.\"\n",
    "        )\n",
    "        return True\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfb409e",
   "metadata": {},
   "source": [
    "##### **b. Store long-term memories**\n",
    "\n",
    "Below is a function that handles storing long-term memories in Redis with built-in deduplication.\n",
    "\n",
    "It's a key part of our memory system that:\n",
    "1. Prevents duplicate memories by checking for similar content\n",
    "2. Creates vector embeddings for semantic search capabilities\n",
    "3. Stores the memory with relevant metadata for future retrieval\n",
    "\n",
    "We'll use the `similar_memory_exists()` function when we store memories in order to perform in-line memory deduplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0d652ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from typing import List, Optional, Union\n",
    "\n",
    "import ulid\n",
    "\n",
    "def store_memory(\n",
    "    content: str,\n",
    "    memory_type: MemoryType,\n",
    "    user_id: str = SYSTEM_USER_ID,\n",
    "    thread_id: Optional[str] = None,\n",
    "    metadata: Optional[str] = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Store a long-term memory in Redis with deduplication\n",
    "\n",
    "    This function:\n",
    "    1. Checks for similar existing memories to avoid duplicates\n",
    "    2. Generates vector embeddings for semantic search\n",
    "    3. Stores the memory with metadata for retrieval\n",
    "    \"\"\"\n",
    "\n",
    "    if metadata is None:\n",
    "        metadata = \"{}\"\n",
    "\n",
    "    logger.info(f\"Preparing to store memory: {content}\")\n",
    "\n",
    "    if similar_memory_exists(content, memory_type, user_id, thread_id):\n",
    "        logger.info(\"Similar memory found, skipping storage\")\n",
    "        return\n",
    "    \n",
    "    embedding = gg_embed.embed_documents([content])\n",
    "    embedding = embedding[0]\n",
    "\n",
    "    memory_data = {\n",
    "        \"user_id\": user_id or SYSTEM_USER_ID,\n",
    "        \"content\": content,\n",
    "        \"memory_type\": memory_type.value,\n",
    "        \"metadata\": metadata,\n",
    "        \"created_at\": datetime.now().isoformat(),\n",
    "        \"embedding\": embedding,\n",
    "        \"memory_id\": str(ulid.ULID()),\n",
    "        \"thread_id\": thread_id\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        long_term_memory_index.load([memory_data])\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error storing memory: {e}\")\n",
    "        return\n",
    "    \n",
    "    logger.info(f\"Stored {memory_type} memory: {content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a3b1c9",
   "metadata": {},
   "source": [
    "##### **c. Retrieve relevant long-term memories**\n",
    "\n",
    "And now that we're storing memories, we can retrieve them using vector similarity search with metadata filters using RedisVL.\n",
    "\n",
    "This function: \n",
    "1. Takes a query string, optional filters (memory type, user ID, thread ID), and a distance threshold (semantic)\n",
    "2. Creates a vector range query using the query's embedding\n",
    "3. Builds a filter object based on passed options\n",
    "4. Filters to narrow down the search results\n",
    "5. Executes the search and returns parsed memory objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "187842f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_memories(\n",
    "    query: str,\n",
    "    memory_type: Union[Optional[MemoryType], List[MemoryType]] = None,\n",
    "    user_id: str = SYSTEM_USER_ID,\n",
    "    thread_id: Optional[str] = None,\n",
    "    distance_threshold: float = 0.1,\n",
    "    limit: int = 5\n",
    ") -> List[StoredMemory]:\n",
    "    \"\"\"\n",
    "    Retrieve relevant memories from Redis using vector similarity search.\n",
    "    \"\"\"\n",
    "\n",
    "    logger.debug(f\"Retrieving memories for query: {query}\")\n",
    "    vector_query = VectorRangeQuery(\n",
    "        vector = gg_embed.embed_query(query),\n",
    "        return_fields = [\n",
    "            \"content\",\n",
    "            \"memory_type\",\n",
    "            \"metadata\",\n",
    "            \"created_at\",\n",
    "            \"memory_id\",\n",
    "            \"thread_id\",\n",
    "            \"user_id\"\n",
    "        ],\n",
    "        num_results = limit,\n",
    "        vector_field_name = \"embedding\",\n",
    "        dialect = 2,\n",
    "        distance_threshold = distance_threshold\n",
    "    )\n",
    "\n",
    "    base_filters = [f\"@user_id:{{{user_id or SYSTEM_USER_ID}}}\"]\n",
    "\n",
    "    if memory_type:\n",
    "        if isinstance(memory_type, list):\n",
    "            base_filters.append(f\"@memory_type:{{{'|'.join(memory_type)}}}\")\n",
    "        else:\n",
    "            base_filters.append(f\"@memory_type:{{{memory_type.value}}}\")\n",
    "    \n",
    "    if thread_id:\n",
    "        base_filters.append(f\"@thread_id:{{{thread_id}}}\")\n",
    "\n",
    "    vector_query.set_filter(\" \".join(base_filters))\n",
    "\n",
    "    # Execute search\n",
    "    results = long_term_memory_index.query(vector_query)\n",
    "\n",
    "    # Parse results into StoredMemory objects\n",
    "    memories = []\n",
    "    for doc in results:\n",
    "        try:\n",
    "            memory = StoredMemory(\n",
    "                id = doc[\"id\"],\n",
    "                memory_id = doc[\"memory_id\"],\n",
    "                user_id = doc[\"user_id\"],\n",
    "                thread_id = doc.get(\"thread_id\", None),\n",
    "                memory_type = MemoryType(doc[\"memory_type\"]),\n",
    "                content = doc[\"content\"],\n",
    "                created_at = doc[\"created_at\"],\n",
    "                metadata = doc[\"metadata\"]\n",
    "            )\n",
    "            memories.append(memory)\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error parsing memory: {e}\")\n",
    "            continue\n",
    "\n",
    "    return memories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1326cb87",
   "metadata": {},
   "source": [
    "### **VI. Managing Long-Term Memory with Tools**\n",
    "\n",
    "Memory operations are exposed as tools that the LLM can call to store or retrieve memories.\n",
    "\n",
    "**Tool-based memory management:**\n",
    "- LLM decided when to store/retrieve memories\n",
    "- Fewer Redis calls but may miss some context\n",
    "- Adds some latency due to LLM decision-making\n",
    "\n",
    "Alternatively, you can always manually manage memories in your workflows.\n",
    "\n",
    "**Manual memory management:**\n",
    "- More Redis calls but faster response times\n",
    "- Extracts more memories, providing richer context\n",
    "- Higher token usage due to more context\n",
    "\n",
    "=> NOTE: This tutorial uses tool-based memory for optimal balance of control and efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23926b11",
   "metadata": {},
   "source": [
    "<img src=\"memory-agents.png\" alt=\"alt text\" width=\"500px\" height=\"400px\" style=\"background-color: lightblue;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c39d275",
   "metadata": {},
   "source": [
    "#### **1. Define Agent Tools**\n",
    "\n",
    "Now that we have our storage functions defined, we can create the tools that will enable our agent to interact with the memory system. These tools will be used by the LLM to manage memories during conversations.\n",
    "\n",
    "Let's start with the Store Memory Tool:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d757337e",
   "metadata": {},
   "source": [
    "##### **a. Store Memory Tool**\n",
    "\n",
    "This tool enables the agent to save important information as long-term memories in Redis. It's particularly useful for capturing:\n",
    "- User preferences and habits\n",
    "- Personal experiences and anecdotes\n",
    "- Important facts and knowledge shared during conversations\n",
    "\n",
    "The tool accepts the following parameters:\n",
    "- `content`: The actual memory content to store (e.g., \"User prefers window seats on flights\")\n",
    "- `memory_type`: The type of memory (e.g., `MemoryType.EPISODIC` for personal experiences, `MemoryType.SEMANTIC` for general knowledge)\n",
    "- `metadata`: Optional dictionary for additional context (e.g., timestamps, source, confidence)\n",
    "- `config`: Optional configuration for user/thread context (automatically handled by the agent)\n",
    "\n",
    "When called, the tool:\n",
    "1. Validates the input parameters\n",
    "2. Stores the memory in Redis with proper indexing\n",
    "3. Returns a success message with the stored content\n",
    "4. Handles errors gracefully with informative messages\n",
    "\n",
    "This tool is designed to be used by the LLM to build a persistent memory of the user's preferences and experiences, enabling more personalized and context-aware interactions overtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "70208852",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Optional\n",
    "\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.runnables.config import RunnableConfig\n",
    "\n",
    "@tool\n",
    "def store_memory_tool(\n",
    "    content: str,\n",
    "    memory_type: MemoryType,\n",
    "    metadata: Optional[Dict[str, str]] = None,\n",
    "    config: Optional[RunnableConfig] = None\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Store a long-term memory in the system.\n",
    "\n",
    "    Use this tool to save important information about user preferences,\n",
    "    experiences, or general knowledge that might be useful in future\n",
    "    interactions.\n",
    "    \"\"\"\n",
    "    config = config or RunnableConfig()\n",
    "    user_id = config.get(\"user_id\", SYSTEM_USER_ID)\n",
    "    thread_id = config.get(\"thread_id\")\n",
    "\n",
    "    try:\n",
    "        store_memory(\n",
    "            content = content,\n",
    "            memory_type = memory_type,\n",
    "            user_id = user_id,\n",
    "            thread_id = thread_id,\n",
    "            metadata = str(metadata) if metadata else None,\n",
    "        )\n",
    "\n",
    "        return f\"Succesfully stored {memory_type} memory: {content}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error storing memory: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f9d00c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15:53:10 __main__ INFO   Preparing to store memory: I like flying on Delta when possible\n",
      "15:53:11 __main__ INFO   Stored MemoryType.EPISODIC memory: I like flying on Delta when possible\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Succesfully stored MemoryType.EPISODIC memory: I like flying on Delta when possible'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store_memory_tool.invoke({\n",
    "    \"content\": \"I like flying on Delta when possible\", \n",
    "    \"memory_type\": \"episodic\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93281daf",
   "metadata": {},
   "source": [
    "##### **b. Retrieve Memories Tool**\n",
    "This tool allows us to search through our stored memories using semantic similarity and filtering.\n",
    "\n",
    "This tool is particularly useful when you want to:\n",
    "- Find relevant past experiences or preferences\n",
    "- Filter memories by type (episodic or semantic)\n",
    "- Get user-specific information\n",
    "- Limit the number of results to keep responses focused\n",
    "\n",
    "The tool works by:\n",
    "1. Taking a query string and searching for semantically similar memories\n",
    "2. Filtering results based on memory type\n",
    "3. Applying a similarity threshold to ensure relevance\n",
    "4. Formatting the results in a clear, readable way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e782995f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def retrieve_memories_tool(\n",
    "    query: str,\n",
    "    memory_type: List[MemoryType],\n",
    "    limit: int = 5,\n",
    "    config: Optional[RunnableConfig] = None\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Retrieve long-term memories relevant to the query.\n",
    "\n",
    "    Use this tool to access previously stored information about user\n",
    "    preferences, experiences, or general knowledge.\n",
    "    \"\"\"\n",
    "    config = config or RunnableConfig()\n",
    "    user_id = config.get(\"user_id\", SYSTEM_USER_ID)\n",
    "\n",
    "    try:\n",
    "        stored_memories = retrieve_memories(\n",
    "            query = query,\n",
    "            memory_type = memory_type,\n",
    "            user_id = user_id,\n",
    "            limit = limit,\n",
    "            distance_threshold = 0.3\n",
    "        )\n",
    "\n",
    "        response = []\n",
    "        if stored_memories:\n",
    "            response.append(\"Long-term memories:\")\n",
    "            for memory in stored_memories:\n",
    "                response.append(f\"- [{memory.memory_type}] {memory.content}\")\n",
    "\n",
    "        return \"\\n\".join(response) if response else \"No relevant memories found.\"\n",
    "    except Exception as e:\n",
    "        return f\"Error retrieving memories: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e2086d12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Long-term memories:\\n- [MemoryType.EPISODIC] I like flying on Delta when possible'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieve_memories_tool.invoke({\n",
    "    \"query\": \"Airline preferences\", \n",
    "    \"memory_type\": [\"episodic\"]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64517398",
   "metadata": {},
   "source": [
    "### **VII. Build the Travel Agent**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89ce484",
   "metadata": {},
   "source": [
    "#### **1. Setting up the ReAct Agent**\n",
    "\n",
    "We'll use LangGraph's prebuilt components to create a ReAct agent with memory capabilities:\n",
    "1. **Short-term Memory**: A checkpoint saver tracks conversation history per thread\n",
    "2. **Long-term Memory**: We'll extract and store key information from conversations\n",
    "- Episodic memories: User preferences and experiences\n",
    "- Semantic memories: General travel knowledge\n",
    "\n",
    "The system will automatically summarize conversations to manage context while preserving important details in long-term storage.\n",
    "\n",
    "Below we start with setting up the Redis checkpointer (`RedisSaver`) that will handle short term memory for the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ce0aac34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16:01:19 langgraph.checkpoint.redis INFO   Redis client is a standalone client\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage, SystemMessage\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langgraph.checkpoint.redis import RedisSaver\n",
    "\n",
    "# Setup the Redis checkpointer for short term memory\n",
    "redis_saver = RedisSaver(redis_client = redis_client)\n",
    "redis_saver.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ceae568e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define set of tools\n",
    "tools = [store_memory_tool, retrieve_memories_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fa7efa1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure an LLM for the agent with a more creative temperature\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model = \"gemini-2.5-flash\",\n",
    "    temperature = 0.7\n",
    ").bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "660c0cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = SystemMessage(\n",
    "    content = \"\"\"\n",
    "    You are a travel assistant helping users plan their trips. You remember user preferences\n",
    "    and provide personalized recommendations based on past interactions.\n",
    "\n",
    "    You have access to the following types of memory:\n",
    "    1. Short-term memory: The current conversation thread\n",
    "    2. Long-term memory:\n",
    "        - Episodic: User preferences and past trip experiences (e.g., \"User prefers window seats\")\n",
    "        - Semantic: General knowledge about travel destinations and requirements\n",
    "\n",
    "    Your procedural knowledge (how to search, book flights, etc.) is built into your tools and prompts.\n",
    "\n",
    "    Always be helpful, personal, and context-aware in your responses.\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Define the travel agent\n",
    "travel_agent = create_react_agent(\n",
    "    model = llm, \n",
    "    tools = tools,\n",
    "    checkpointer = redis_saver,\n",
    "    prompt = system_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f391c3d9",
   "metadata": {},
   "source": [
    "#### **2. Build nodes of LangGraph workflow**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4ec9db",
   "metadata": {},
   "source": [
    "##### **a. Node 1: Respond to the user**\n",
    "\n",
    "In LangGraph, a node represents a discrete unit of processing in a workflow. Each node is a function that takes a state object and configuration as input, processes the data, and returns an updated state. Nodes can be connected to from a directed graph that defines the flow of execution.\n",
    "\n",
    "The `respond_to_user` node is our first node in the travel agent workflow. It serves as the entry point for user interactions and handles the core conversation flow.\n",
    "Here's how it works:\n",
    "1. It receives the current conversation state and configuration\n",
    "2. Extracts any human messages from the state\n",
    "3. Invokes our travel agent to generate a repsonse\n",
    "4. Handles any errors gracefully\n",
    "5. Updates the conversation state with the agent's response\n",
    "\n",
    "The node uses a custom `RuntimeState` class that inherits from `MessagesState` to maintain the conversation history. This state object is passed between nodes in the graph, allowing each node to access and modify the conversation context as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c66e3a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph.message import MessagesState\n",
    "\n",
    "class RuntimeState(MessagesState):\n",
    "    \"\"\"Runtime state for the travel agent.\"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cb764efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "def respond_to_user(\n",
    "    state: RuntimeState,\n",
    "    config: RunnableConfig\n",
    ") -> RuntimeState:\n",
    "    \"\"\"Invoke the travel agent to generate a response.\"\"\"\n",
    "    human_messages = [m for m in state[\"messages\"]\n",
    "                      if isinstance(m, HumanMessage)]\n",
    "    \n",
    "    if not human_messages:\n",
    "        logger.warning(\"No HumanMessage found in state\")\n",
    "        return state\n",
    "    \n",
    "    try:\n",
    "        # Single agent invocation, not streamed (simplified for reliablity)\n",
    "        result = travel_agent.invoke(\n",
    "            input = {\"messages\": state[\"messages\"]},\n",
    "            config = config\n",
    "        )\n",
    "        agent_message = result[\"messages\"][-1]\n",
    "        state[\"messages\"].append(agent_message)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error invoking travel agent: {e}\")\n",
    "        agent_message = AIMessage(\n",
    "            content = \"I'm sorry, I encountered an error processing your request.\"\n",
    "        )\n",
    "        state[\"messages\"].append(agent_message)\n",
    "    \n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ec199d",
   "metadata": {},
   "source": [
    "##### **b. Node 2: Execute Tools**\n",
    "\n",
    "The `execute_tools` node is a critical component in our travel agent's workflow that bridges the gap between the LLM's decisions and actual tool execution. Positioned after the `respond_to_user` node, it handles the pratical side of the agent's tool-using capabilities.\n",
    "\n",
    "When the LLm determines it needs to use a tool, it includes tool calls in its response. This node then:\n",
    "1. Scans the conversation history to find the most recent AI message containing tool calls\n",
    "2. For each tool call found:\n",
    "- Extracts the tool name, arguments and call ID from message\n",
    "- Matches the tool name against our available tools\n",
    "- Executes the tool with the provided arguments\n",
    "- Creates a ToolMessage containing the result\n",
    "3. Handles any errors that occur during tool execution\n",
    "4. Adds all tool results back to the conversation history\n",
    "\n",
    "This node is essential because it enables our agent to interact with external systems and services while maintaining a coherent conversation flow. Without it, the agent would be limited to just generating text responses without the ability to perform actual actions or retrieve real-time information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "736c68ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import ToolMessage\n",
    "\n",
    "def execute_tools(\n",
    "    state: RuntimeState,\n",
    "    config: RunnableConfig\n",
    ") -> RuntimeState:\n",
    "    \"\"\"Execute tools specified in the latest AIMessage and append ToolMessages.\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    latest_ai_message = next(\n",
    "        (\n",
    "            m for m in reversed(messages) \n",
    "            if isinstance(m, AIMessage) and m.tool_calls\n",
    "        ),\n",
    "        None\n",
    "    )\n",
    "\n",
    "    if not latest_ai_message:\n",
    "        return state # No tool calls to process\n",
    "    \n",
    "    tool_messages = []\n",
    "    for tool_call in latest_ai_message.tool_calls:\n",
    "        tool_name = tool_call[\"name\"]\n",
    "        tool_args = tool_call[\"args\"]\n",
    "        tool_id = tool_call[\"id\"]\n",
    "\n",
    "        # Find the corresponding tool\n",
    "        tool = next((t for t in tools if t.name == tool_name), None)\n",
    "        if not tool:\n",
    "            continue # Skip if tool not found\n",
    "\n",
    "        try:\n",
    "            # Execute the tool with the provided arguments\n",
    "            result = tool.invoke(tool_args, config=config)\n",
    "\n",
    "            # Create a ToolMessage with the result\n",
    "            tool_message = ToolMessage(\n",
    "                content = str(result),\n",
    "                tool_call_id = tool_id,\n",
    "                name = tool_name\n",
    "            )\n",
    "            tool_messages.append(tool_message)\n",
    "        except Exception as e:\n",
    "            error_message = ToolMessage(\n",
    "                content = f\"Error executing tool '{tool_name}': {str(e)}\",\n",
    "                tool_call_id = tool_id,\n",
    "                name = tool_name\n",
    "            )\n",
    "            tool_messages.append(error_message)\n",
    "\n",
    "        # Append the ToolMessages to the message history\n",
    "        messages.extend(tool_messages)\n",
    "        state[\"messages\"] = messages\n",
    "        return state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a80591",
   "metadata": {},
   "source": [
    "##### **c. Node 3: Conversation Summarization**\n",
    "\n",
    "The conversation summarization node helps manage context by condensing chat history into concise summaries. This prevents the LLM's context window from being overwhelmed as the conversation grows.\n",
    "\n",
    "Key features:\n",
    "1. **Automatic Triggering**: Summarizes after every 6 messages (configurable)\n",
    "2. **Smart Summarization**:\n",
    "- Uses `gemini-2.5-flash` with low temperature (0.3) for consistent summaries\n",
    "- Preserves key information like preferences and pending items\n",
    "- Replaces old messages while keeping recent context\n",
    "3. **Benefits**:\n",
    "- Prevents context window overflow\n",
    "- Maintains conversation coherence\n",
    "- Optimizes token usage\n",
    "\n",
    "The summary becomes part of the conversation history allow the agent to reference past interactions efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0ec6f138",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import RemoveMessage\n",
    "\n",
    "# An LLM configured for summarization\n",
    "summarizer = ChatGoogleGenerativeAI(model = \"gemini-2.5-flash\", temperature = 0.3)\n",
    "\n",
    "# The number of messages after which we'll summarize the conversation\n",
    "MESSAGE_SUMMARIZATION_THRESHOLD = 6\n",
    "\n",
    "\n",
    "def summarize_conversation(\n",
    "    state: RuntimeState,\n",
    "    config: RunnableConfig\n",
    ") -> RuntimeState:\n",
    "    \"\"\"\n",
    "    Summarize a list of messages into a concise summary to reduce context length\n",
    "    while preserving important information.\n",
    "    \"\"\"\n",
    "\n",
    "    messages = state[\"messages\"]\n",
    "    current_message_count = len(messages)\n",
    "    if current_message_count < MESSAGE_SUMMARIZATION_THRESHOLD:\n",
    "        logger.debug(f\"Not summarizing conversation: {current_message_count}\")\n",
    "        return state\n",
    "\n",
    "    system_prompt = \"\"\"\n",
    "    You are a conversation summarizer. Create a concise summary of the previous\n",
    "    conversation between a user and a travel assistant.\n",
    "\n",
    "    The summary should:\n",
    "    1. Highlight key topics, preferences, and decisions\n",
    "    2. Include any specific trip details (destinations, dates, preferences)\n",
    "    3. Note any outstanding questions or topics that need follow-up\n",
    "    4. Be concise but informative\n",
    "\n",
    "    Format your summary as a brief narrative paragraph.\n",
    "    \"\"\"\n",
    "\n",
    "    message_content = '\\n'.join(\n",
    "        [\n",
    "            f\"{'User' if isinstance(msg, HumanMessage) else 'Assistant'}: {msg.content}\"\n",
    "            for msg in messages\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    summary_messages = [\n",
    "        SystemMessage(content = system_prompt),\n",
    "        HumanMessage(\n",
    "            content = f\"Please summarize this conversation:\\n\\n{message_content}\"\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    summary_response = summarizer.invoke(summary_messages)\n",
    "\n",
    "    logger.info(f\"Summarized {len(messages)} messages into a conversation summary\")\n",
    "\n",
    "    summary_message = SystemMessage(\n",
    "        content = f\"\"\"\n",
    "        Summary of the conversation so fat:\n",
    "\n",
    "        {summary_response.content}\n",
    "\n",
    "        Please continue the conversation based on this summary and the recent messages.\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    remove_messages = [\n",
    "        RemoveMessage(id = msg.id) for msg in messages if msg.id is not None\n",
    "    ]\n",
    "\n",
    "    state[\"messages\"] = [\n",
    "        *remove_messages,\n",
    "        summary_message,\n",
    "        state[\"messages\"][-1]\n",
    "    ]\n",
    "\n",
    "    return state.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2907e23",
   "metadata": {},
   "source": [
    "#### **3. Assemble the full graph**\n",
    "\n",
    "It's time to assemble our graph for end-to-end agent execution. We will attach all 3 nodes we defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d619d158",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x2d7dcd3b4d0>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "workflow = StateGraph(RuntimeState)\n",
    "\n",
    "# Add nodes to the graph\n",
    "workflow.add_node(\"agent\", respond_to_user)\n",
    "workflow.add_node(\"execute_tools\", execute_tools)\n",
    "workflow.add_node(\"summarize_conversation\", summarize_conversation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bfb790",
   "metadata": {},
   "source": [
    "Next, we''ll tie the nodes together using edges which control process flow. There's a conditional edge between the agent node and what comes next. What comes next is based on whether we need to handle + execute a tool call or proceed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1ff01df6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x2d7dcd3b4d0>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def decide_next_step(state):\n",
    "    latest_ai_message = next(\n",
    "        (\n",
    "            m for m in reversed(state[\"messages\"])\n",
    "            if isinstance(m, AIMessage)\n",
    "        ),\n",
    "        None\n",
    "    )\n",
    "    if latest_ai_message and latest_ai_message.tool_calls:\n",
    "        return \"execute_tools\"\n",
    "    return \"summarize_conversation\"\n",
    "\n",
    "workflow.set_entry_point(\"agent\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    decide_next_step,\n",
    "    {\n",
    "        \"execute_tools\": \"execute_tools\",\n",
    "        \"summarize_conversation\": \"summarize_conversation\"\n",
    "    }\n",
    ")\n",
    "workflow.add_edge(\"execute_tools\", \"agent\")\n",
    "workflow.add_edge(\"summarize_conversation\", END)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6ac4efae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiled the graph\n",
    "graph = workflow.compile(checkpointer=redis_saver)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5671c9ea",
   "metadata": {},
   "source": [
    "### **VII. Testing the Main Agent Loop**\n",
    "\n",
    "Let's put our travel agent to work! The main loop handles the conversation flow:\n",
    "1. **Initialization**: Sets up a unique thread ID and empty message state\n",
    "2. **Input Processing**: Gets user input, handles empty inputs and exit commands\n",
    "3. **Message Flow**: Converts input to HumanMessage and streams through our workflow\n",
    "4. **Reponse Generation**: Processes state and displays AI responses\n",
    "5. **Error Handling**: Catches errors and keeps the conversation flowing smoothly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9ef5fdf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(\n",
    "    thread_id: str = \"book_flight\",\n",
    "    user_id: str = \"demo_user\"\n",
    "):\n",
    "    \"\"\"Main interaction loop for the travel agent\"\"\"\n",
    "    print(\"Welcome to the Travel Assistant! (Type 'exit' to quit)\")\n",
    "\n",
    "    config = RunnableConfig(\n",
    "        configurable = {\n",
    "            \"thread_id\": thread_id,\n",
    "            \"user_id\": user_id\n",
    "        }\n",
    "    )\n",
    "    state = RuntimeState(messages = [])\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\"\\nYou (type 'quit' to quit): \")\n",
    "\n",
    "        if not user_input:\n",
    "            continue\n",
    "\n",
    "        if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "            print(\"Thank you for using the Travel Assistant. Goodbye!\")\n",
    "            break\n",
    "\n",
    "        state[\"messages\"].append(HumanMessage(content=user_input))\n",
    "\n",
    "        try:\n",
    "            # Process user input through the graph\n",
    "            for result in graph.stream(state, config=config, stream_mode=\"values\"):\n",
    "                state = RuntimeState(**result)\n",
    "\n",
    "            logger.debug(f\"# of messages after run: {len(state[\"messages\"])}\")\n",
    "\n",
    "            # Find the most recent AI message, so we can print the repsonse\n",
    "            ai_messages = [m for m in state[\"messages\"] if isinstance(m, AIMessage)]\n",
    "            if ai_messages:\n",
    "                message = ai_messages[-1].content\n",
    "            else:\n",
    "                logger.error(\"No AI messages after run\")\n",
    "                message = \"I'm sorry, I couldn't process your request properly.\"\n",
    "\n",
    "                # Add the error message to the state\n",
    "                state[\"messages\"].append(AIMessage(content=message))\n",
    "\n",
    "            print(f\"\\nAssistant: {message}\")\n",
    "        except Exception as e:\n",
    "            logger.exception(f\"Error processing request: {e}\")\n",
    "            error_message = \"I'm sorry, I encountered an error processing your request.\"\n",
    "            print(f\"\\nAssistant: {error_message}\")\n",
    "            # Add the error message to the state\n",
    "            state[\"messages\"].append(AIMessage(content=error_message))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cfe7b3",
   "metadata": {},
   "source": [
    "Before you try your own, take a look at the current conversation between Tyler and the travel agent. Notice the memory storage actions, the calls to the LLM, and also the conversation summarization that take plact during the workflow!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "beca2a8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the Travel Assistant! (Type 'exit' to quit)\n",
      "\n",
      "Assistant: That sounds like a fantastic trip! Singapore is an amazing choice for both outdoor activities and incredible food experiences.\n",
      "\n",
      "For **outdoors**, you absolutely have to visit:\n",
      "\n",
      "*   **Gardens by the Bay**: Explore the Supertree Grove, Cloud Forest, and Flower Dome. It's a stunning futuristic park that's beautiful both day and night.\n",
      "*   **Singapore Botanic Gardens**: A UNESCO World Heritage Site, perfect for a leisurely stroll amidst lush greenery, including the National Orchid Garden.\n",
      "*   **MacRitchie Reservoir Park**: If you're up for a bit more adventure, you can go for a hike, including the TreeTop Walk, which offers a panoramic view of the forest canopy.\n",
      "\n",
      "And for **food**, you're in for a treat! Singapore is a culinary paradise:\n",
      "\n",
      "*   **Hawker Centers**: This is where you'll find the heart of Singaporean cuisine. I highly recommend trying **Lau Pa Sat** for its vibrant atmosphere and satay street at night, and **Maxwell Food Centre** for famous dishes like Tian Tian Hainanese Chicken Rice.\n",
      "*   **Peranakan Cuisine**: This unique fusion of Chinese and Malay flavors is a must-try. Look for restaurants in the Katong/Joo Chiat area.\n",
      "*   **Little India and Kampong Glam**: Explore these vibrant neighborhoods for authentic Indian and Malay/Middle Eastern dishes.\n",
      "\n",
      "To help me remember for future recommendations, would you like me to store that you and your wife love outdoors activities and trying new kinds of foods?\n",
      "17:42:10 __main__ INFO   Preparing to store memory: User and wife love outdoors activities.\n",
      "17:42:10 __main__ INFO   Preparing to store memory: User and wife love trying new kinds of foods.\n",
      "17:42:11 __main__ INFO   Stored MemoryType.EPISODIC memory: User and wife love outdoors activities.\n",
      "17:42:11 __main__ INFO   Stored MemoryType.EPISODIC memory: User and wife love trying new kinds of foods.\n",
      "\n",
      "Assistant: Great! I've made a note that you and your wife love outdoor activities and trying new foods. This will help me tailor future recommendations for you.\n",
      "\n",
      "Is there anything else I can help you with for your Singapore trip, or perhaps another destination?\n",
      "17:42:49 __main__ INFO   Summarized 6 messages into a conversation summary\n",
      "\n",
      "Assistant: I don't have specific information on the \"best\" flight routes from Atlanta to Singapore readily available in my memory at the moment. However, I can tell you that there are no direct flights from Atlanta (ATL) to Singapore (SIN). You'll typically have one or two layovers.\n",
      "\n",
      "Common layover cities for this route include:\n",
      "\n",
      "*   **In Asia:** Tokyo (NRT/HND), Seoul (ICN), Hong Kong (HKG), Taipei (TPE)\n",
      "*   **In Europe:** Amsterdam (AMS), Paris (CDG), Frankfurt (FRA)\n",
      "*   **In the Middle East:** Doha (DOH), Dubai (DXB)\n",
      "\n",
      "Airlines that frequently operate these routes include Delta, Singapore Airlines, Qatar Airways, Emirates, Cathay Pacific, EVA Air, and Korean Air, among others.\n",
      "\n",
      "To find the absolute best routes for your specific travel dates and preferences (e.g., shortest travel time, fewest layovers, preferred airline alliance), I would recommend using a flight search engine like Google Flights, Skyscanner, or Kayak.\n",
      "\n",
      "Would you like me to store this general information about ATL to SIN routes for future reference?\n",
      "\n",
      "Assistant: Given that you're looking for recommendations on flight paths from Atlanta to Singapore, here are the most common and often recommended layover cities, keeping in mind there are no direct flights:\n",
      "\n",
      "1.  **Via Asia:** This is often the most direct in terms of overall travel time, though still a long journey.\n",
      "    *   **Tokyo (NRT/HND):** A very popular layover with good connections. Airlines like Delta partner with airlines that fly to Tokyo, or you can fly a Star Alliance carrier.\n",
      "    *   **Seoul (ICN):** Another excellent hub with good connections to Singapore. Korean Air and Asiana Airlines are major carriers here.\n",
      "    *   **Hong Kong (HKG):** Cathay Pacific is a major airline for this route.\n",
      "    *   **Taipei (TPE):** EVA Air is a highly-rated airline that flies this route.\n",
      "\n",
      "2.  **Via the Middle East:** These routes often involve highly-rated airlines and modern airports.\n",
      "    *   **Doha (DOH):** Qatar Airways is a top-tier airline with excellent service, and Doha is a very efficient hub.\n",
      "    *   **Dubai (DXB):** Emirates is another excellent choice, offering good connections to Singapore.\n",
      "\n",
      "3.  **Via Europe:** While possible, these routes generally add more travel time compared to Asian or Middle Eastern layovers for a destination like Singapore.\n",
      "    *   **Amsterdam (AMS), Paris (CDG), Frankfurt (FRA):** These are major European hubs, but flying through them from Atlanta to Singapore usually means a longer overall journey.\n",
      "\n",
      "**My top recommendation for the shortest overall travel time would generally be to look for flights with a single layover in a major Asian hub like Tokyo, Seoul, or Hong Kong, or a Middle Eastern hub like Doha or Dubai.**\n",
      "\n",
      "Remember, the \"best\" path can also depend on factors like airline preference, layover duration, and price. I highly recommend using a flight search engine to compare options for your specific travel dates.\n",
      "17:45:25 __main__ INFO   Summarized 6 messages into a conversation summary\n",
      "\n",
      "Assistant: This sounds like a wonderful idea! Based on your preferences for outdoor activities and trying new foods, here's a perfect Sunday itinerary for you and your wife in Singapore:\n",
      "\n",
      "---\n",
      "\n",
      "**A Perfect Singapore Sunday: Nature, Culture & Culinary Delights**\n",
      "\n",
      "**Morning (9:00 AM - 12:00 PM): Gardens by the Bay - Nature's Masterpiece**\n",
      "\n",
      "*   Start your day with a refreshing visit to **Gardens by the Bay**. Given your love for outdoor activities, you'll be captivated by this futuristic garden. Take a leisurely stroll through the outdoor themed gardens, marvel at the iconic Supertrees, and enjoy the stunning views of Marina Bay. If you're up for it, consider visiting the Cloud Forest or Flower Dome for a truly unique experience, blending nature with innovative design.\n",
      "\n",
      "**Lunch (12:30 PM - 2:00 PM): Hawker Haven at Maxwell Food Centre**\n",
      "\n",
      "*   Immerse yourselves in Singapore's vibrant food scene with lunch at **Maxwell Food Centre**. This renowned hawker center is a fantastic spot to try a variety of local delicacies. You absolutely must try the famous Hainanese Chicken Rice (Tian Tian is a popular stall!), along with other local favorites like char kway teow, satay, or a refreshing sugarcane juice. It's an authentic and delicious culinary adventure!\n",
      "\n",
      "**Afternoon (2:30 PM - 5:30 PM): Peranakan Charm in Katong/Joo Chiat**\n",
      "\n",
      "*   After lunch, head to the charming **Katong and Joo Chiat neighborhood**. This area is a treasure trove of Peranakan heritage, known for its beautifully preserved, colorful shophouses and unique architecture. Enjoy a leisurely walk, admire the intricate designs, and soak in the cultural atmosphere. This is also a great opportunity to pop into a local bakery to try some delicious Peranakan \"kueh\" (traditional snacks and desserts).\n",
      "\n",
      "**Late Afternoon (5:30 PM - 7:00 PM): Sunset Views at Marina Barrage**\n",
      "\n",
      "*   Before dinner, make your way to **Marina Barrage**. This is a lovely spot to relax outdoors, enjoy the cool breeze, and take in panoramic views of the city skyline, the Supertrees, and Marina Bay. It's especially beautiful as the sun begins to set, offering fantastic photo opportunities.\n",
      "\n",
      "**Evening (7:30 PM onwards): A Peranakan Culinary Journey**\n",
      "\n",
      "*   Conclude your perfect Sunday with an exquisite **Peranakan dinner**. There are several excellent Peranakan restaurants in the Katong area (or other parts of Singapore) where you can delve deeper into this unique cuisine. Savor dishes like Ayam Buah Keluak (chicken with black nut), Babi Pongteh (braised pork with fermented soybean sauce), or a rich Laksa. It's the perfect way to cap off a day of outdoor exploration and new food experiences!\n",
      "\n",
      "---\n",
      "\n",
      "This itinerary blends iconic outdoor attractions with authentic culinary experiences and cultural exploration, all at a comfortable pace for a Sunday. I hope you and your wife have an absolutely wonderful time!\n",
      "Thank you for using the Travel Assistant. Goodbye!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    user_id = input(\"Enter a user ID: \") or \"demo_user\"\n",
    "    thread_id = input(\"Enter a thread ID: \") or \"demo_thread\"\n",
    "except Exception as e:\n",
    "    exit()\n",
    "else:\n",
    "    main(thread_id, user_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f2970eec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Long-term memories:',\n",
       " '- [MemoryType.EPISODIC] I like flying on Delta when possible']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = retrieve_memories_tool.invoke({\"query\": \"\", \"memory_type\": [\"episodic\", \"semantic\"]})\n",
    "res.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0d85fad1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from redisvl.query import CountQuery\n",
    "\n",
    "# count total long-term memories in Redis\n",
    "long_term_memory_index.query(CountQuery())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed463b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-agent-tutorials",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
